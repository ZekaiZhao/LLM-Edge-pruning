# -*- coding: utf-8 -*-
"""Number_of_nodes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Csb0Lg-foyryao8V3Z9doqdEl4GJqkWk
"""

import torch
import os
from transformers import AutoConfig
from modeling_fllama import FLlamaForCausalLM

def show_heads_and_mlp(checkpoint_path, device="cuda"):
    """
    Loads the pruned model from `checkpoint_path` and prints the
    number of heads alive and whether the MLP node is kept (1) or pruned (0)
    for each layer.
    """
    # 1. Load config
    config = AutoConfig.from_pretrained(checkpoint_path)
    # 2. Load model with the same dtype/device that was used
    model = FLlamaForCausalLM.from_pretrained(
        checkpoint_path,
        config=config,
        torch_dtype=torch.float16,  # or 'bf16', or 'float32'
        device_map=None,
    ).to(device)

    # 3. Fetch node masks
    node_masks = model.get_node_masks()
    num_layers = config.num_hidden_layers
    num_heads = config.num_attention_heads

    # if you used embedding pruning:
    with_embeds = model.model.with_embedding_nodes
    offset = 0

    # Prepare lists to store layer-level info
    heads_alive_per_layer = []
    mlp_status_per_layer = []

    # 4. Iterate over each layer
    for layer_idx in range(num_layers):
        # node_masks[layer_idx + offset] => (z_attn, z_mlp)
        z_attn, z_mlp = node_masks[layer_idx + offset]

        # Count how many heads remain (heads with mask > 0.0)
        # Often we just sum them because each dimension = 1 or 0
        # But if your code uses real-values (0.<mask<1.), you might threshold:
        #    heads_alive = (z_attn > 0.5).sum().item()
        # For deterministic pruning, summation is usually fine:
        heads_alive = int(torch.sum(z_attn).item())

        # MLP block is typically a scalar in [0,1]
        # If it's close to 1 => MLP block is kept. If 0 => pruned.
        mlp_kept = float(z_mlp.item())

        heads_alive_per_layer.append(heads_alive)
        mlp_status_per_layer.append(mlp_kept)

    # 5. Print out results
    total_heads_alive = sum(heads_alive_per_layer)
    total_mlp_kept = sum(1 for x in mlp_status_per_layer if x > 0.0)

    print("=" * 70)
    print(f" Pruned Model from: {checkpoint_path}")
    print(f"  -> Number of layers: {num_layers}")
    print(f"  -> Heads (per layer): {heads_alive_per_layer}")
    print(f"  -> MLP  (per layer): {[round(x, 4) for x in mlp_status_per_layer]}")
    print(f"     => Total heads alive: {total_heads_alive} / {num_layers * num_heads}")
    print(f"     => Total MLP blocks:  {total_mlp_kept} / {num_layers}")
    print("=" * 70)

    # Cleanup (especially if large model on GPU)
    del model
    torch.cuda.empty_cache()

# Example usage:
if __name__ == "__main__":
    outputs_dir = "/data/zekai/zzk_project_llm1/1b_pruning/1b_pruning/outputs"
    checkpoint_names = [
        "checkpoint-5000",
    ]

    for ckpt in checkpoint_names:
        ckpt_path = os.path.join(outputs_dir, ckpt)
        show_heads_and_mlp(ckpt_path, device="cuda")
# -*- coding: utf-8 -*-
"""LLM_project_code2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16BOftlmPxEYZ3qZeqY7FhlyND87cqh7F
"""

import logging
import sys
import os
import warnings
from dataclasses import dataclass, field
from typing import Optional

import torch
import transformers
from transformers import (
    HfArgumentParser,
    Seq2SeqTrainingArguments,
    set_seed,
)

# Import the pruning-enabled model and trainer from your snippet
from modeling_fllama import FLlamaForCausalLM
from fllama_boolean_expressions_fs import FLlamaTrainer
#from your_data_loading_script import load_datasets, DataCollatorBool  # Adjust as needed

logger = logging.getLogger(__name__)

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from tqdm import tqdm
import numpy as np
import json
import math
import copy
from torch.nn.utils import clip_grad_norm_
import matplotlib.pyplot as plt
import re

import torch
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch

from transformers import AutoConfig

# model_name_or_path was previously defined and should be left unchanged
model_name_or_path = 'meta-llama/Llama-3.2-1B-Instruct'

# Load the tokenizer
#tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)

# Ensure the tokenizer has a pad token before creating the DataLoader
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '<pad>'})

def data_collator(features):
    # input_ids = [torch.tensor(f["input_ids"], dtype=torch.long) for f in features]
    # attention_mask = [torch.tensor(f["attention_mask"], dtype=torch.long) for f in features]
    # labels = [torch.tensor(f["labels"], dtype=torch.long) for f in features]

    input_ids = [f["input_ids"].clone().detach().long() for f in features]
    attention_mask = [f["attention_mask"].clone().detach().long() for f in features]
    labels = [f["labels"].clone().detach().long() for f in features]


    # Handle missing corr_input_ids
    corr_input_ids = [torch.tensor(f.get("corr_input_ids", [tokenizer.pad_token_id]), dtype=torch.long) for f in features]

    idxes = [torch.tensor(f.get("idxes", 0), dtype=torch.long) for f in features]  # Default idx to 0 if missing

    # Pad sequences to the maximum length within the batch
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = pad_sequence(labels, batch_first=True, padding_value=-100)
    corr_input_ids = pad_sequence(corr_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    idxes = torch.stack(idxes, dim=0)  # No padding needed for idxes

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "corr_input_ids": corr_input_ids,
        "idxes": idxes
    }

# Get the config first
config = AutoConfig.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

# Ensure 'rope_scaling' is in the config and has a 'type' key
# Use a default 'type' if not present
config.rope_scaling = config.rope_scaling or {}  # Create if not exists
config.rope_scaling["type"] = config.rope_scaling.get("type", "linear")  # Set to 'linear' if not found

# Load the model from a pretrained checkpoint or a local directory
model = FLlamaForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",  # Example model name or local path
    config=config,
    with_embedding_nodes=False,          # Adjust based on your setup
    disable_linear_regularization_term=False
)

print("Edge Sparsity:", model.get_edge_sparsity())
print("Node Sparsity:", model.get_node_sparsity())

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '<pad>'})
model.resize_token_embeddings(len(tokenizer))
# If you're using pruning logic and have reference model:
# llama_model = FLlamaForCausalLM.from_pretrained("some_ref_model_checkpoint")

import torch
from torch.nn.utils.rnn import pad_sequence

# include corr_idx, and idexes
# update the preprocess function
import random
# Preprocessing function
def preprocess_gsm8k(example, tokenizer, max_length=128):
    question = example["question"]
    answer = example["answer"]

    # Tokenize question and answer separately
    question_enc = tokenizer(
        question,
        add_special_tokens=False,
        truncation=True,
        max_length=max_length
    )
    answer_enc = tokenizer(
        answer,
        add_special_tokens=False,
        truncation=True,
        max_length=max_length
    )

    # Combine question and answer tokens
    input_ids = question_enc["input_ids"] + answer_enc["input_ids"]
    attention_mask = [1] * len(input_ids)  # All tokens are real tokens

    # Create labels: -100 for the question tokens and actual IDs for the answer
    labels = [-100] * len(question_enc["input_ids"]) + answer_enc["input_ids"]

    # Identify the start index of the answer tokens in input_ids
    idxes = len(question_enc["input_ids"])

    # Generate corrupted input (perturbed question)
    corrupted_question = perturb_question(question)
    corr_question_enc = tokenizer(
        corrupted_question,
        add_special_tokens=False,
        truncation=True,
        max_length=max_length
    )

    corr_input_ids = corr_question_enc["input_ids"] + answer_enc["input_ids"]
        # Ensure all token IDs are within the valid range
    # This check is crucial to avoid the IndexError
    # input_ids = [id for id in input_ids if id < tokenizer.vocab_size]
    # labels = [label if label < tokenizer.vocab_size else -100 for label in labels]

    # Truncate to max_length if necessary
    if len(input_ids) > max_length:
        input_ids = input_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        labels = labels[:max_length]

        # Adjust idxes if it exceeds the truncated sequence
        if idxes >= max_length:
            idxes = max_length - 1

    if len(corr_input_ids) > max_length:
        corr_input_ids = corr_input_ids[:max_length]

    # Ensure all token IDs are within the valid range
    input_ids = [id for id in input_ids if id < tokenizer.vocab_size]
    labels = [label if label < tokenizer.vocab_size else -100 for label in labels]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "corr_input_ids": corr_input_ids,
        "idxes": idxes
    }

def perturb_question(question):
    """
    Introduces slight perturbations to the question for generating corr_input_ids.
    Example perturbations: shuffle words, mask a random word, etc.
    """
    words = question.split()
    if len(words) > 1:
        # Shuffle the order of words slightly
        random.shuffle(words)
    return " ".join(words)


# Prepare the Processed Dataset and Data Collator: You should have something like this already done:
# define train and test data
gsm8k = load_dataset("openai/gsm8k", "main", cache_dir="/home/zekai/.cache/huggingface/datasets")
max_length = 128
train_data = gsm8k["train"].map(lambda x: preprocess_gsm8k(x, tokenizer=tokenizer, max_length=max_length), batched=False)
# Include 'corr_input_ids', "idxes" in the columns list
train_data.set_format(type="torch", columns=["input_ids", "attention_mask", "labels", "corr_input_ids", "idxes"])

test_data = gsm8k["test"].map(lambda x: preprocess_gsm8k(x, tokenizer=tokenizer, max_length=max_length), batched=False)
# Include 'corr_input_ids', "idxes" in the columns list
test_data.set_format(type="torch", columns=["input_ids", "attention_mask", "labels", "corr_input_ids", "idxes"])

train_loader = DataLoader(train_data, batch_size=1, shuffle=True, collate_fn=data_collator)
test_loader = DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=data_collator)


from transformers.utils import logging
logging.set_verbosity_info()

from transformers import Seq2SeqTrainingArguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./outputs",
    overwrite_output_dir=True,
    num_train_epochs= 3,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    logging_steps= 1,
    save_steps=1000,
    evaluation_strategy="steps",
    eval_steps=1000,
    load_best_model_at_end=True,
    disable_tqdm=False
)

target_edge_sparsity = 1.2
target_node_sparsity = 0.7

# Get the config first
config = AutoConfig.from_pretrained("meta-llama/Llama-3.2-1B-Instruct")

# Ensure 'rope_scaling' is in the config and has a 'type' key
# Use a default 'type' if not present
config.rope_scaling = config.rope_scaling or {}  # Create if not exists
config.rope_scaling["type"] = config.rope_scaling.get("type", "linear")  # Set to 'linear' if not found

# Load the model from a pretrained checkpoint or a local directory
llama_model = FLlamaForCausalLM.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",  # Example model name or local path
    config=config
)

print("Edge Sparsity at init:", llama_model.get_edge_sparsity())
print("Node Sparsity at init:", llama_model.get_node_sparsity())

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '<pad>'})
llama_model.resize_token_embeddings(len(tokenizer))

# Correct usage:
from accelerate import Accelerator

# Initialize once at the very beginning
accelerator = Accelerator(mixed_precision="bf16")

from transformers import TrainerCallback
# Pass the same accelerator instance to FLlamaTrainer
trainer = FLlamaTrainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    data_collator=data_collator,
    start_edge_sparsity=0.0,
    target_edge_sparsity=target_edge_sparsity,
    start_node_sparsity=0.0,
    target_node_sparsity=target_node_sparsity,
    skip_node_loss_if_higher_sparsity=False,
    num_sparsity_warmup_steps=200, # 200
    warmup_type="linear",
    edges_lr= 0.8,    # 0.8
    nodes_lr= 0.8,       # 0.8
    reg_edges_lr=0.4, # 0.4
    reg_nodes_lr= 0.4,    # 0.4
    warmup_steps=200, # needed 200
    disable_node_loss=False,
    llama_model=llama_model
)

class SparsityCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        # Access trainer safely
        trainer = kwargs.get("trainer", None)
        if trainer is None:
            print("[SparsityCallback Warning] Trainer instance not found in kwargs.")
            return control  # Return the unmodified control object
        model = trainer.model
        edge_s = model.get_edge_sparsity()
        node_s = model.get_node_sparsity()
        print(f"[Step {state.global_step}] Edge: {edge_s:.4f}, Node: {node_s:.4f}")

trainer.add_callback(SparsityCallback())

trainer.train()
final_edge_sparsity = model.get_edge_sparsity()
final_node_sparsity = model.get_node_sparsity()
print("Edge Sparsity after training:", final_edge_sparsity)
print("Node Sparsity after training:", final_node_sparsity)

metrics = trainer.evaluate()
print("Eval metrics:", metrics)

edge_s_final = model.get_edge_sparsity()
node_s_final = model.get_node_sparsity()
print("Final Edge Sparsity:", edge_s_final)
print("Final Node Sparsity:", node_s_final)